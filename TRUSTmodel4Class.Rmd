---
title: "Trust Model"
date: "`r paste('Last edits on:',date(), sep=' ')`"
output: 
  html_document:
    toc: true
    number_sections: false
    toc_float: true
    df_print: paged
    code_folding: show
  pdf_document: default
  html_notebook: default
---

```{r preamble, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	background = "#ADD8E6",
	cache = TRUE,
	tidy = 'styler',
	class.source = "bg-warning",
	class.output = "bg-success",
	fig_caption = TRUE,
	digits = 3
)
```

```{r libraries, include=FALSE}
using<-function(...) {
    libs<-unlist(list(...))
    req<-unlist(lapply(libs,require,character.only=TRUE))
    need<-libs[req==FALSE]
    if(length(need)>0){ 
        install.packages(need)
        lapply(need,require,character.only=TRUE)
    }
}
using("tidyverse","ggplot2","psych","corrplot","DT","formatR","ggExtra","car","rethinking","rstan","dagitty","brms","bayestestR")

round_df <- function(df, digits) {
  nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
  df[,nums] <- round(df[,nums], digits = digits)
  (df)
}
```

# The Model to Test

I have in mind two models - one that represents trust as a latent variable that I will compute outside of my stan code and one that represents trust as an emergent variable.  Why treat them differently?  Good question.  There is a tension in the literature that posits trust to be either latent or emergent.  Instead of worrying about how to handle it in the class, I opted to do two models and compare them.  Why not?

## Model 1:  Trust as a latent variable

```{r TModel1}
## I copied http://www.daggitty.net code into the daggitty function below
Tdag1 <- dagitty('dag {"Beh Int" [outcome,pos="-0.272,-0.078"]
"SR Trust" [pos="-0.327,-0.077"]
GI [pos="-0.398,-0.082"]
Rel [pos="-0.402,-0.077"]
Trust [latent,pos="-0.367,-0.077"]
Unc [pos="-0.402,-0.072"]
"SR Trust" -> "Beh Int"
Trust -> "SR Trust"
Trust -> GI
Trust -> Rel
Trust -> Unc
}
')
plot(Tdag1)
impliedConditionalIndependencies(Tdag1)
```

## Model 2: Trust as an emergent variable

```{r TModel2}
Tdag2 <- dagitty('dag {"Beh Int" [outcome,pos="-0.272,-0.078"]
"SR Trust" [pos="-0.327,-0.077"]
GI [pos="-0.398,-0.082"]
Rel [pos="-0.402,-0.077"]
Trust [outcome,pos="-0.367,-0.077"]
Unc [pos="-0.402,-0.072"]
"SR Trust" -> "Beh Int"
Trust -> "SR Trust"
Trust <- GI
Trust <- Rel
Trust <- Unc
}
')
plot(Tdag2)
impliedConditionalIndependencies(Tdag2)
```

# The Data

I have over 8 studies of trust data; for this class, I will use only one study to make life simpler.  The data come from a fully crossed design whereby participants read vignettes and rated the vignettes based upon several facets and then were asked to rate how much they would trust the agent in the vignette.  Our aim was to better understand the factors that lead to people indicating trust.  

```{r TData}
Tdat <- read.csv("/home/pem725/GoogleDrive/00 Most Current Papers/Trust - MRES New Model/Data/dat5alll.csv",header=T)[,-1]
Tdat$B <- abs(Tdat$B-2) # now B = 1 for yes and B = 0 for no
describe(Tdat)
```

So I have B for my DV and G, U1, R as my manifest indicators of trust (the construct) that lead to predicting B through T (self-reported trust).  The model seems complicated but it is not really.  For each model, I need to create new variables.

## Model 1 (Latent Trust) Data Management

```{r M1Data, warning=FALSE}
## added data step here for efa
pcor1 <- cor(Tdat[,c(3,4,6)],use="complete.obs")
Tdat.lwcN <- nrow(Tdat[complete.cases(Tdat[,c(3,4,6)]),])
efa1 <- fa(Tdat[,c(3,4,6)],1)
#Tdat$tx3.efa1
Tdat$tx3.efa1 <- factor.scores(Tdat[,c(3,4,6)],efa1)$scores
ggplot(Tdat,aes(tx3.efa1)) + geom_density()
```

## Model 2 (Emergent Trust) Data Management

```{r M2Data, message=FALSE, warning=FALSE}

pc1 <- principal(pcor1,1,residuals=TRUE,n.obs=Tdat.lwcN, scores = TRUE)
pc1.wts <- as.data.frame(pc1$weights)
str(pc1.wts)
#pc1.wts
Tdat$tx3.pc1 <- pc1.wts$PC1[1]*scale(Tdat$G) + pc1.wts$PC1[2]*scale(Tdat$U1) + pc1.wts$PC1[3]*scale(Tdat$R)
Tdat <- Tdat[!is.na(Tdat$tx3.pc1),]
ggplot(Tdat,aes(tx3.pc1)) + geom_histogram()

```

# Model Priors

We need to model priors for each of the parameters of interest.  To simplify my modeling, I intend to use the same priors for each model.  Thus, I only need to "envision" the betas (regression parameters or log odds ratios) for one model and use it for both.  



## DV Priors

I have a binary DV (Behavioral Intent to Trust the Agent) that is coded as "1" for the person indicating the intent to trust the agent and as "0" for the the person indicating the intent NOT to trust the agent.  Given that coding, I envision a beta distribution with the following parameters:

```{r DVpriors, message=FALSE, warning=FALSE}
# Assume a fixed N for hypothetical reasons:

DVprior.dat <- data.frame(NTrust=0:16,
                          P.NTrust=0:16/16,
                          ExpCts=c(2,3,6,8,10,12,13,12,9,8,5,4,3,2,1,1,1)
                          )
DVprior.dat$TotCts <- DVprior.dat$NTrust*DVprior.dat$ExpCts
DVprior.dat$DensTotCts <- DVprior.dat$TotCts/sum(DVprior.dat$TotCts)
DVprior.dat$ExpBeta <- dbeta((0:16/16),sum(DVprior.dat$TotCts),sum(DVprior.dat$ExpCts)*16 - sum(DVprior.dat$TotCts))
#DVprior.dat$Scenes <- as.factor(DVprior.dat$Scenes)
#sum(DVprior.dat$ExpCts)
DVprior.dat
p1 <- ggplot(DVprior.dat,aes(x=P.NTrust,y=(ExpCts/sum(ExpCts))*16)) + geom_point()
p1 + geom_line(aes(x=P.NTrust,y=ExpBeta))
```

The priors for my DV are a bit cryptic but they work just fine.  My priors are pretty strong (i.e., regularlizing for sure) so I wonder how much I will learn from the data.  Perhaps a ton.  When we shift to modeling, I will probably change priors from these strong priors to more naive priors such as Jeffreys' priors (i.e., beta(.5,.5)).  I will use the beta function for my priors even though it does not really reflect my hunch.  

## Reassessing Things

My results so far had me questioning the use of a beta distribution when modeling my dependent variable.  As a result, I chose to step back and start reading a bit more about various alternatives.  Here were my resources:

1. [The Beta-Binomial Bayesian Model](https://www.bayesrulesbook.com/chapter-3.html)
2. [Simulation Based Calibration (SBC) with the Beta-Binomial Model](https://cran.r-project.org/web/packages/rstan/vignettes/SBC.html)
3. [Bernoulli Logistic Model](https://discourse.mc-stan.org/t/assessing-priors-for-bernoulli-logistic-model/14864)
4. [Validating Baysian Inference Algorithms with SBC](https://arxiv.org/pdf/1804.06788.pdf)
5. and then....I went down an internet rabbit hole of [Rasch models](https://mc-stan.org/docs/2_29/stan-users-guide/item-response-models.html) and such without much headway.

Tuesday, I will regale you all with my exploits and where I stand.

Thursday comes with some added complications:

1. DV is binary but my real model is a mediation
2. Binary DV results in LR or LLR but my a path will be a regular b.
3. Mixing LR with b's is a bit dodgy so I needed to look into this further.
HINT1:  [Mediation with brms](https://discourse.mc-stan.org/t/mediation-question-using-brms/4260)
HINT2:  


## BRMS models

I now shift my attention to coming up with priors for my paths between variables.  Each model has two paths that I must estimate.  


```{r BRMSmodels, message=FALSE, warning=FALSE}
efa.fit <- brm(B ~ T * tx3.efa1 + (1 + T * tx3.efa1 | scen), 
               data=Tdat, family = bernoulli, 
               prior = c(set_prior("normal(0, 1)", class = "Intercept"),
               prior("normal(0, 1)", class = "b")))
summary(efa.fit, waic=TRUE)
#hypothesis(efa.fit, "")
#stancode(efa.fit)
pca.fit <- brm(B ~ T * tx3.pc1 + (1 + T * tx3.pc1 | scen), 
               data=Tdat, family = bernoulli, 
               prior = c(prior("normal(0, 1)", class = "Intercept"),
               prior("normal(0, 1)", class = "b")))
summary(pca.fit, waic=TRUE)


```


## Model Comparison

```{r ModelComp}
 LOO(efa.fit,pca.fit)
```

